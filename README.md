# Podcast Persona Debates - Agentic AI Hackathon

> **AI agents trained on podcast personalities engage in multi-agent debates with real-time fact-checking**

## ğŸ¯ Project Overview

This project creates AI agents based on popular podcast host personalities (Joe Rogan, Ezra Klein, Lex Fridman) that can engage in dynamic debates. Each agent is fine-tuned on podcast transcripts using Gemma models and orchestrated through FastAPI for multi-agent interactions.

## ğŸ§  Core Concept

- **Fine-tune lightweight Gemma models** on podcast transcripts to capture each host's unique style, opinions, and speaking patterns
- **Multi-agent orchestration** using FastAPI to facilitate natural debates between personalities  
- **Real-time fact-checking** using structured podcast verification data as a critic agent
- **Chain-of-Thought reasoning** for complex discussions and nuanced position-taking
- **Optional voice synthesis** to bring the debates to life

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Joe Agent  â”‚     â”‚ Ezra Agent â”‚     â”‚ Lex Agent  â”‚
â”‚ (Gemma +   â”‚     â”‚ (Gemma +   â”‚     â”‚ (Gemma +   â”‚
â”‚ LoRA-Joe)  â”‚     â”‚ LoRA-Ezra) â”‚     â”‚ LoRA-Lex)  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚                  â”‚
 Personality         Personality       Personality
 Prompting           Prompting         Prompting
     â”‚                  â”‚                  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[ FastAPI Server ]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†•
                Fact-checking / Critic Agent
                         â†•
              [ Voice Synthesis (Optional) ]
```

## ğŸ› ï¸ Tech Stack

### Models & Training
- **Base Model**: Google Gemma 1.1 (2B/7B instruction-tuned)
- **Fine-tuning**: LoRA/QLoRA adapters for memory efficiency
- **Training Framework**: ğŸ¤— Transformers + PEFT + bitsandbytes

### Infrastructure 
- **Deployment**: Google Cloud Run with NVIDIA L4 GPUs
- **API Framework**: FastAPI for multi-agent workflows
- **Monitoring**: Opik by Comet for agent evaluation and safety
- **Voice**: Coqui TTS/XTTS (optional)

### Data
- **3,066 training examples** from 21 real Joe Rogan podcast episodes
- Structured fact-checking data for verification
- Chain-of-thought reasoning datasets

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install torch transformers datasets peft accelerate bitsandbytes trl opik
```

### ğŸ‰ Live Gemma API Endpoint

Our pre-deployed Gemma 3-1B model is ready for use:

- **Service URL**: https://gemma-1b-wamyzspxga-ew.a.run.app
- **Model**: gemma3:1b
- **Region**: europe-west1
- **Status**: âœ… Working (tested with "Why is the sky blue?" query)

#### ğŸ”§ API Usage
```bash
curl -X POST "https://gemma-1b-wamyzspxga-ew.a.run.app/api/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:1b",
    "prompt": "Your question here"
  }'
```

#### Example Usage
```bash
# Test the API with a simple question
curl -X POST "https://gemma-1b-wamyzspxga-ew.a.run.app/api/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:1b",
    "prompt": "Explain quantum computing in simple terms"
  }'
```

### Quick Start with Real Data
```bash
# 1. Data is already processed and ready
ls data/joe_transcripts.json  # 3066 training examples

# 2. Test training pipeline (dry run)
python3 test_training_dry_run.py

# 3. Train models (requires GPU/ML packages)
./scripts/run_training.sh

# 4. Test locally
uvicorn api.server:app --host 0.0.0.0 --port 8080
python3 test_api.py

# 5. Deploy to Cloud Run
./scripts/deploy_cloudrun.sh
```

### Training Options
```bash
# Quick test with small dataset
python3 scripts/train_lora.py --persona joe --data data/joe_transcripts.json --output models/joe --epochs 1

# Full training
./scripts/run_training.sh
```

## ğŸ“Š Dataset Structure

Real Joe Rogan training data format:
```json
{
  "prompt": "You are Joe Rogan, a curious and skeptical podcast host...\n\nQuestion: What's your take on this?",
  "response": "You know, I think people need to really question what they're being told here...",
  "persona": "joe_rogan",
  "episode": "Joe Rogan Experience #2349 - Danny Jones",
  "episode_id": "89IEFOiW-Z0"
}
```

## ğŸ¯ Hackathon Goals

- [x] **Team Formation** (2-5 people, ML/multi-agent expertise)
- [x] **Real Data Collection** - 3,066 examples from 21 Joe Rogan episodes
- [x] **Training Pipeline** - LoRA fine-tuning scripts ready
- [x] **API Server** - FastAPI with inference and debate endpoints
- [x] **Testing Framework** - Dry run validation without GPU requirements
- [ ] **Model Training** - Fine-tune Gemma agents on Google Cloud GPU
- [ ] **Agent Orchestration** - Multi-agent debate implementation
- [ ] **Demo Preparation** - Live debate showcase with optional voice
- [ ] **Submission** - Deploy on Cloud Run, submit by 10:00 AM Day 2

## ğŸ§ª Testing

The project includes comprehensive testing without requiring ML packages:

```bash
# Test training pipeline logic
python3 test_training_dry_run.py

# Test with full dataset
python3 test_training_dry_run.py --data data/joe_transcripts.json

# Process local transcripts
python3 scripts/process_local_transcripts.py
```

## ğŸ† Demo Scenarios

1. **Cross-ideological Debates** - "What would Joe Rogan say to Ezra Klein about AI regulation?"
2. **Multi-perspective Analysis** - Three agents analyzing complex topics from different angles
3. **Fact-checked Discussions** - Real-time verification of claims during debates
4. **Voice-enabled Conversations** - Audio output for immersive experience

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ joe_transcripts.json     # 3066 Joe Rogan training examples
â”‚   â”œâ”€â”€ lex_transcripts.json     # Sample Lex Fridman data
â”‚   â””â”€â”€ joe_rogan_complete.json  # Full processed dataset
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ joe/                     # Joe Rogan LoRA adapter
â”‚   â””â”€â”€ lex/                     # Lex Fridman LoRA adapter
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server.py                # FastAPI inference server
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_lora.py            # LoRA training script
â”‚   â”œâ”€â”€ process_local_transcripts.py # Real data processing
â”‚   â”œâ”€â”€ run_training.sh          # Training automation
â”‚   â””â”€â”€ deploy_cloudrun.sh       # Cloud Run deployment
â”œâ”€â”€ test_training_dry_run.py     # Training pipeline validation
â”œâ”€â”€ test_api.py                  # API testing script
â”œâ”€â”€ Dockerfile                   # Cloud Run deployment
â””â”€â”€ requirements.txt             # Dependencies
```

## ğŸ“ˆ Dataset Statistics

- **Episodes**: 21 Joe Rogan podcast episodes
- **Training Examples**: 3,066 real conversation segments
- **Average Length**: 522 characters per response
- **Characteristic Phrases**: "you know", "bro", "that's crazy", "Jamie, pull that up"
- **Content Types**: Conversations, debates, interviews, monologues

## ğŸ‘¥ Team

Looking for teammates with:
- **ML/Fine-tuning experience** (LoRA, model training)
- **Multi-agent systems** (FastAPI, agent orchestration)  
- **Voice synthesis** (TTS, voice cloning)
- **Cloud deployment** (Docker, Cloud Run)

## ğŸ“š Resources

- [Google Cloud Run GPU Handbook](https://cloud.google.com/run/docs/configuring/services/gpu)
- [Opik Agent Observability](https://github.com/comet-ml/opik)
- [Gemma Model Hub](https://huggingface.co/google/gemma-1.1-2b-it)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

**Built for the Agentic AI App Hackathon** | **Powered by Google Cloud Run + NVIDIA L4 GPUs**